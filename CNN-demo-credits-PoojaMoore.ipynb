{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D19021 - Pooja More - Feed Forward Network for MNIST Classification in Pytorch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hejgZ6TnkKMh"
      },
      "source": [
        "## **D19021 - Pooja More - Feed Forward Network for MNIST Classification in Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za12b8bv1gWr"
      },
      "source": [
        "### Objective :\n",
        "To build a Feed Forward Network for MNIST Classification in Pytorch in not more than 10 epochs.\n",
        "\n",
        "### Parameters :\n",
        "1. Number of parameters used in the model ( lower the better)\n",
        "2. Validation data accuracy (higher the better)\n",
        "3. Experimentation details to reach at the final set of parameters used in the model.\n",
        "4. Uploading your code on your github profile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm2XXzQT3Ub7"
      },
      "source": [
        "**Solution** :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPysM75msanM"
      },
      "source": [
        "Importing necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqc5qpRp_fVY"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets,transforms\n",
        "from torch import optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1KvviPisfee"
      },
      "source": [
        "Uploading the train and test data using dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsTRVnem_gGX"
      },
      "source": [
        "transform=transforms.Compose([transforms.ToTensor()])\n",
        "trainset=datasets.MNIST('~/.pytorch/MNIST_data/',train=True,transform=transform,download=True)\n",
        "testset=datasets.MNIST('~/.pytorch/MNIST_data/',train=False,transform=transform,download=True)\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=100,shuffle=True,num_workers=0)\n",
        "#will explain later\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=100,shuffle=True,num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsrQ4Hhh26W_"
      },
      "source": [
        "Defining the Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr96eKLL_gIv"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 100)  #Input Layer\n",
        "        self.fc2 = nn.Linear(100, 50)   #Hidden Layer 1\n",
        "        #self.fc3 = nn.Linear(64, 50)   #Hidden Layer 2\n",
        "        self.fc4 = nn.Linear(50, 10)    #Output Layer\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "      '''Dropout to avoid overfitting the model onto the training data'''\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        #x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "      '''Softmax function, a wonderful activation function that turns numbers aka logits into probabilities that sum to one.\n",
        "      Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.\n",
        "      The algorithms leverages the power of adaptive learning rates methods to find individual learning rates for each parameter.\n",
        "      It also has advantages of Adagrad, which works really well in settings with sparse gradients'''\n",
        "\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "model=Network()\n",
        "#Adam Optimizer\n",
        "'''Adam - adaptive learning rate optimization algorithm\n",
        "Adam actually finds worse solution than stochastic gradient descent.\n",
        "'''\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#SGD - Stochastic Gradient Descent Optimizer\n",
        "'''Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function\n",
        "with suitable smoothness properties (e.g. differentiable or subdifferentiable).\n",
        "It can be regarded as a stochastic approximation of gradient descent optimization,\n",
        "since it replaces the actual gradient (calculated from the entire data set) by an estimate\n",
        "thereof (calculated from a randomly selected subset of the data).'''\n",
        "\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001,momentum=0.9)\n",
        "\n",
        "#Negative Log Likelihood\n",
        "'''The negative log-likelihood loss function is often used in combination with a SoftMax activation function\n",
        "to define how well your neural network classifies data.\n",
        "The negative log-likelihood function is defined as loss=-log(y) and produces a high value when the values of the output layer\n",
        "are evenly distributed and low. In other words, there's a high loss when the classification is unclear.\n",
        "It also produces relative high values when the classification is wrong. '''\n",
        "\n",
        "criterion=nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWby6Tll_gNZ",
        "outputId": "7331c406-67c6-408c-e00a-f20ecea61789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "epochs=7\n",
        "train_losses,test_losses=[],[]\n",
        "for e in range(epochs):\n",
        "    running_loss=0\n",
        "    for images,labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        images=images.view(images.shape[0],-1)\n",
        "        log_ps=model(images)\n",
        "        loss=criterion(log_ps,labels) # a single value for ex 2.33\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.shape[0] ## (2.33*64 + 2.22*64 + 2.12*33) / 138\n",
        "\n",
        "    else:\n",
        "        test_loss=0\n",
        "        accuracy=0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for images,labels in testloader:\n",
        "                log_ps=model(images)\n",
        "                test_loss+=criterion(log_ps,labels) *images.shape[0]\n",
        "                ps=torch.exp(log_ps)\n",
        "                top_p,top_class=ps.topk(1,dim=1)\n",
        "                equals=top_class==labels.view(*top_class.shape)\n",
        "                accuracy+=torch.sum(equals).item()\n",
        "        model.train()\n",
        "        train_losses.append(running_loss/len(trainloader.dataset))\n",
        "        test_losses.append(test_loss.item()/len(testloader.dataset))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader.dataset)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader.dataset)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader.dataset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/7..  Training Loss: 0.065..  Test Loss: 0.077..  Test Accuracy: 0.978\n",
            "Epoch: 2/7..  Training Loss: 0.062..  Test Loss: 0.077..  Test Accuracy: 0.977\n",
            "Epoch: 3/7..  Training Loss: 0.062..  Test Loss: 0.076..  Test Accuracy: 0.979\n",
            "Epoch: 4/7..  Training Loss: 0.058..  Test Loss: 0.076..  Test Accuracy: 0.979\n",
            "Epoch: 5/7..  Training Loss: 0.055..  Test Loss: 0.081..  Test Accuracy: 0.978\n",
            "Epoch: 6/7..  Training Loss: 0.052..  Test Loss: 0.076..  Test Accuracy: 0.980\n",
            "Epoch: 7/7..  Training Loss: 0.053..  Test Loss: 0.083..  Test Accuracy: 0.978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfIXeXhtzROC"
      },
      "source": [
        "Calculating the total number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBhXhQyPSJSk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "d38a27fd-e2a0-4966-efe8-e49cda2c34dd"
      },
      "source": [
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "pytorch_total_params"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model: \n",
            "\n",
            " Network(\n",
            "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (fc4): Linear(in_features=50, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ") \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84060"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxH1Yyy6zfzU"
      },
      "source": [
        "**Conclusion :**\n",
        "\n",
        "As per the observations recorded in excel sheet we can find that as the number of layers increases, apparently the number of neurons increases and so does the number of parameters. So we have purposely avoided including hidden layers into our model.\n",
        "<br>So far the best accuracy we have got is **98.0** using the RELU activation function using Adam optimizer having the neurons distribution as follows :-\n",
        "<br>Input Layer : 784\n",
        "<br>Hidden Layer : 50\n",
        "<br>Output Layer : 10\n",
        "<br><br>The specifications are as follows :-\n",
        "<br>Batch Size : 100\n",
        "<br>Learning Rate :  0.001\n",
        "<br>Dropout : 0.2\n",
        "<br>No. of Epochs : 7\n",
        "\n",
        "<br>The accuracy of the model can be increased if we increase the number of hidden layers and the number of neurons as well as the batch size.\n",
        "\n",
        "<br> ***For now I can conclude that this is so far the best optimized solution I have found.***"
      ]
    }
  ]
}